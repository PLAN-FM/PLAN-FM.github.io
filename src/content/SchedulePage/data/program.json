[
    {
        "id": "break",
        "time": "9:00",
        "title": "Opening Remarks",
        "hasResource": "yes",
        "resource": "slides",
        "resourceLink": "./1_OpeningRemarks.pdf"
    },
     {
        "id": "tutorial",
        "time": "9:10",
        "title": "Tutorial",
        "topic": "A Brief Introduction to AI Planning, by Shirin Sohrabi",
        "hasResource": "yes",
        "resource": "slides",
        "resourceLink": "./2_Tutorial-Planning.pdf"
    },{
        "id": "tutorial",
        "time": "9:30",
        "title": "Tutorial",
        "topic": "A Primer on Large Language Models, by Soham Dan",
        "hasResource": "yes",
        "resource": "slides",
        "resourceLink": "./3_Tutorial_LLMs.pdf"
    },{
        "id": "tutorial",
        "time": "9:50",
        "title": "Tutorial",
        "topic": "Benchmarks for Planning in Natural Language, by Harsha Kokel",
        "hasResource": "yes",
        "resource": "slides",
        "resourceLink": "./4_Tutorial_Benchmarks.pdf"
    },
    {
        "id": "break",
        "time": "10:30",
        "title": "Coffee Break",
        "speaker": ""
    },
    {
        "id": "tutorial",
        "time": "11:00",
        "title": "Tutorial",
        "topic": "Embodied Agents"
    },
    {
        "id": "expand",
        "time": "11:40",
        "title": "Invited Talk",
        "speaker": "Michael Katz",
        "topic": "Large Language Models for AI Planning: Efficiency and correctness through code generation",
        "abstract": "Among the most important properties of algorithms investigated in computer science are soundness, completeness, and complexity. These properties, however, are rarely analyzed for the vast collection of recently proposed methods for planning with large language models. In this work, we alleviate this gap. We analyze these properties of using LLMs for planning and highlight that recent trends abandon both soundness and completeness for the sake of inefficiency. We propose a significantly more efficient approach, Thought of Search (ToS), that can, at the same time, maintain both soundness and completeness. We exemplify on four representative search problems, comparing to the LLM-based solutions from the literature that attempt to solve these problems. We show that by using LLMs to produce the code for the search components we can solve the entire datasets with 100% accuracy with only a few calls to the LLM. The caveat is that ToS requires a human in the loop, collaboratively producing a sound successor function and goal test. To overcome this limitation, in this work, we make a first major step towards automating ToS (AutoToS), completely taking the human out of the loop of interactions with the language model. AutoToS guides the language model step by step towards the generation of sound and complete search components, through feedback from both generic and domain specific unit tests. AutoToS achieves 100% accuracy, with a small number of feedback iterations, using LLMs of various sizes on all evaluated domains."
    },
    {
        "id": "talk",
        "time": "12:10",
        "title": "Contributed Talk",
        "topic": "Generic-to-Specific Reasoning and Learning for Ad Hoc Teamwork",
        "speaker": ""
    },
    {
        "id": "talk",
        "time": "12:20",
        "title": "Contributed Talk",
        "topic": "L2P: A Python Toolkit for Automated PDDL Model Generation with Large Language Models",
        "speaker": ""
    },
    {
        "id": "break",
        "time": "12:30",
        "title": "Lunch with a Mentor",
        "speaker": "",
        "hasResource": "yes",
        "resource": "recommended places",
        "resourceLink": "https://aaai.org/conference/aaai/aaai-25/aaai-25-walkable-dining/"
    },
    {
        "id": "talk",
        "time": "1:55",
        "title": "Contributed Talk",
        "topic": "FixMyPlan: Leveraging Large Language Models to Fix Ill-Defined Models and Incorrect Plans",
        "speaker": ""
    },
    {
        "id": "talk",
        "time": "2:05",
        "title": "Contributed Talk",
        "topic": "From Semantic Understanding to Geometric Features: Using Foundation Models for Novel Robotic Tasks",
        "speaker": ""
    },
    {
        "id": "expand",
        "time": "2:15",
        "title": "Invited Talk",
        "speaker": "   Mak Roberts",
"abstract": "This talk highlights my long-running research vision that I call Conversational Companions, which are agents that can interact with humans in dialog while also tracking historical context of the relationship over time. After providing an “escalator pitch” where two friendly colleagues reconnect while taking an escalator to the next session, I discuss elements that will be required for such companions.  I frame the challenge problem from the perspective of logic, cognitive architectures, and model manipulation and I tie themes back to the bridge program.  Furthermore, I describe some gotchas that I think need more work.  With these framing concepts, I present a list of possible requirements. ",
"topic":"Conversational Companions: A Challenge Problem"
    },
    {
        "id": "expand-noabs",
        "time": "2:45",
        "title": "Distinguished Talk",
        "speaker": "   Karen Myers",
        "topic": "AI Planning: From Shakey Foundations to a Chatty Future",
        "abstract": ""
    },
    {
        "id": "talk",
        "time": "3:20",
        "title": "Contributed Talk",
        "topic": "Towards a More Rigorous PDDL Generation Benchmark, or, (:goal (benchmark pddl_generators))",
        "speaker": ""
    },
    {
        "id": "break",
        "time": "3:30",
        "title": "Coffee Break",
        "speaker": ""
    },
    {
        "id": "expand-noabs",
        "time": "4:00",
        "title": "Panel",
        "topic": "Planning in Natural Language with LLMs",
        "speaker": "Michael Katz, Karen Myers, Mak Roberts, Swaroop Mishra (Remotely)"
    },
    {
        "id": "break",
        "time": "5:00",
        "title": "Closing Remarks",
        "speaker": ""
    }
]